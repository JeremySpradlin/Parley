# Parley Analytics Dashboard: Implementation Plan

This document outlines the plan for creating a new analytics dashboard in the Parley application. The plan is designed to be executed by an AI developer like Claude.

## 1. Overview & Goals

The objective is to create an analytics dashboard that provides deep insights into the AI-to-AI conversations generated by Parley. This will move beyond simple metrics to analyze the *content* and *dynamics* of the interactions.

**Key Features:**

*   **Content Analysis**: Understand what the AIs are talking about (sentiment, topics).
*   **Linguistic Metrics**: Analyze the complexity and style of the language used.
*   **Interaction Dynamics**: Measure the back-and-forth between the participants.
*   **Performance Metrics**: Track response times.

## 2. Backend Implementation (FastAPI)

We will create a new set of services and endpoints to handle the analysis of completed conversations.

### 2.1. New Dependencies

The following libraries should be added to `backend/requirements.txt` to support Natural Language Processing (NLP):

```
nltk==3.8.1
textblob==0.18.0.post0
textstat==0.7.3
wordcloud==1.9.3
```

After updating `requirements.txt`, run `pip install -r backend/requirements.txt`. You will also need to download the necessary NLTK data. Create a small Python script or run in a shell:
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```

### 2.2. New API Endpoints

Two new endpoints will be added to `backend/app/api/conversation.py`:

1.  **`GET /api/conversations`**
    *   **Purpose**: To provide the frontend with a list of all conversations (and their statuses) currently in memory. This will populate a dropdown for selecting a conversation to analyze.
    *   **Response**: A JSON list of objects, each with `id`, `status`, and a descriptive `name` based on the personas.
    *   **Implementation**: This endpoint should iterate through the `CONVERSATIONS` dictionary from `app.state` and return a summary for each.

2.  **`GET /api/analytics/{conversation_id}`**
    *   **Purpose**: To return a full analytics report for a single, specified conversation.
    *   **Response Model**: A detailed JSON object (`ConversationAnalytics`) containing all the computed metrics. (See section 2.4 for the model definition).
    *   **Implementation**: This endpoint will fetch the specified conversation from the `CONVERSATIONS` state, pass it to the new `AnalyticsService`, and return the result. It should handle cases where the conversation is not found.

### 2.3. New Service: `AnalyticsService`

*   A new file, `backend/app/services/analytics_service.py`, will be created to house the core analysis logic.
*   The main class, `AnalyticsService`, will have a public method `analyze_conversation(conversation)`.
*   This method will orchestrate the analysis by performing the following steps:
    *   **Sentiment Analysis**: Calculate polarity (-1 to 1) and subjectivity (0 to 1) for each message using `TextBlob`.
    *   **Topic/Keyword Extraction**: Combine all messages, tokenize, remove common "stop words" (`nltk`), and calculate the frequency of the remaining words to identify key topics. Return the top 30-50 words.
    *   **Readability Analysis**: Calculate the Flesch-Kincaid grade level for the entire conversation text using `textstat`.
    *   **Linguistic Diversity**: Calculate the Type-Token Ratio (unique words / total words), a measure of vocabulary richness.
    *   **Interaction Metrics**: Count messages per speaker, calculate the average response time between turns, and determine the ratio of questions asked by each AI (e.g., by counting messages ending in "?").

### 2.4. New Data Models

To ensure type safety and clear API contracts, a new file `backend/app/models/analytics.py` will be created to define the Pydantic models for the analytics data.

```python
# backend/app/models/analytics.py
from pydantic import BaseModel
from typing import List, Dict

class SentimentPoint(BaseModel):
    message_index: int
    sentiment_polarity: float
    sentiment_subjectivity: float
    speaker: str

class WordFrequency(BaseModel):
    text: str
    value: int

class ConversationAnalytics(BaseModel):
    conversation_id: str
    sentiment_over_time: List[SentimentPoint]
    topic_keywords: List[WordFrequency]
    readability_score: float
    vocabulary_richness: float
    message_counts: Dict[str, int]
    avg_response_time_seconds: float
    question_ratio: Dict[str, float]
```

## 3. Frontend Implementation (Next.js/React)

A new section of the application will be built to consume and display the analytics data.

### 3.1. New Dependencies

Run the following command to add the necessary frontend libraries:

```bash
npm install recharts react-wordcloud
```

### 3.2. New Page & Route

*   A new page will be created at `/analytics` by creating the directory and file `frontend/src/app/analytics/page.tsx`.
*   A link to this new "Analytics" page should be added to the main application layout (`frontend/src/app/components/ConversationView.tsx` or another central navigation component) for easy access.

### 3.3. New Components

The analytics page will be built from several modular components, likely located in `frontend/src/app/components/analytics/`.

*   **`AnalyticsDashboard.tsx`**: The main parent component for the page. It will:
    1.  Fetch the list of available conversations from `GET /api/conversations` on mount.
    2.  Display a dropdown (`<select>`) allowing the user to choose a conversation.
    3.  When a conversation is selected, fetch the detailed report from `GET /api/analytics/{conversation_id}`.
    4.  Manage loading and error states.
    5.  Render child components, passing the analytics data down as props.

*   **`SentimentChart.tsx`**: A line chart (`recharts`) that plots the sentiment polarity of each AI's messages over the course of the conversation. It should render two lines, one for each speaker.

*   **`WordCloud.tsx`**: A component to visualize the most frequent keywords using `react-wordcloud`.

*   **`MetricsCard.tsx`**: A reusable card to display individual statistics. It could take `title`, `value`, and `tooltip` as props to display metrics like "Readability Score," "Avg. Response Time," etc.

This structured plan should provide a clear path for implementing the analytics dashboard. 